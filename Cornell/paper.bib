@inproceedings{you2025origami,
  author    = {Wang, Jiyang and You, Yuchen and Zhang, Xinqi and Fang, Haobo and Wang, Jiaqi and Huang, Xiaonan},
  title     = {Origami {I}nspired {S}oft {R}obotic {A}rm: {A} {M}odular {P}latform for {M}anipulation},
  booktitle = {The International Conference on Robotics and Automation (ICRA) 2025 Workshop},
  year      = {2025},
  month     = may
}

@inproceedings{10.1145/3651890.3672248,
author = {Amir, Daniel and Saran, Nitika and Wilson, Tegan and Kleinberg, Robert and Shrivastav, Vishal and Weatherspoon, Hakim},
title = {Shale: A Practical, Scalable Oblivious Reconfigurable Network},
year = {2024},
isbn = {9798400706141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3651890.3672248},
doi = {10.1145/3651890.3672248},
abstract = {Circuit-switched technologies have long been proposed for handling high-throughput traffic in datacenter networks, but recent developments in nanosecond-scale reconfiguration have created the enticing possibility of handling low-latency traffic as well. The novel Oblivious Reconfigurable Network (ORN) design paradigm promises to deliver on this possibility. Prior work in ORN designs achieved latencies that scale linearly with system size, making them unsuitable for large-scale deployments. Recent theoretical work showed that ORNs can achieve far better latency scaling, proposing theoretical ORN designs that are Pareto optimal in latency and throughput.In this work, we bridge multiple gaps between theory and practice to develop Shale, the first ORN capable of providing low-latency networking at datacenter scale while still guaranteeing high throughput. By interleaving multiple Pareto optimal schedules in parallel, both latency- and throughput-sensitive flows can achieve optimal performance. To achieve the theoretical low latencies in practice, we design a new congestion control mechanism which is best suited to the characteristics of Shale. In datacenter-scale packet simulations, our design compares favorably with both an in-network congestion mitigation strategy, modern receiver-driven protocols such as NDP, and an idealized analog for sender-driven protocols. We implement an FPGA-based prototype of Shale, achieving orders of magnitude better resource scaling than existing ORN proposals. Finally, we extend our congestion control solution to handle node and link failures.},
booktitle = {Proceedings of the ACM SIGCOMM 2024 Conference},
pages = {449-464},
numpages = {16},
keywords = {optical switches, datacenter networks, nanosecond switching},
location = {Sydney, NSW, Australia},
series = {ACM SIGCOMM '24}
}

@inproceedings {306005,
author = {Jiaxin Lin and Zhiyuan Guo and Mihir Shah and Tao Ji and Yiying Zhang and Daehyeok Kim and Aditya Akella},
title = {{E}nabling {P}ortable and {High-Performance} {SmartNIC} Programs with Alkali},
booktitle = {22nd USENIX Symposium on Networked Systems Design and Implementation (NSDI 25)},
year = {2025},
isbn = {978-1-939133-46-5},
address = {Philadelphia, PA},
pages = {107--126},
url = {https://www.usenix.org/conference/nsdi25/presentation/lin-jiaxin},
publisher = {USENIX Association},
month = apr
}

@inproceedings {286477,
author = {Jiaxin Lin and Adney Cardoza and Tarannum Khan and Yeonju Ro and Brent E. Stephens and Hassan Wassel and Aditya Akella},
title = {{RingLeader}: {E}fficiently {O}ffloading {Intra-Server} {O}rchestration to {NICs}},
booktitle = {20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)},
year = {2023},
isbn = {978-1-939133-33-5},
address = {Boston, MA},
pages = {1293--1308},
url = {https://www.usenix.org/conference/nsdi23/presentation/lin},
publisher = {USENIX Association},
month = apr
}

@inproceedings{10.1145/3651890.3672257,
author = {Krentsel, Alexander and Saran, Nitika and Koley, Bikash and Mandal, Subhasree and Narayanan, Ashok and Ratnasamy, Sylvia and Al-Shabibi, Ali and Shaikh, Anees and Shakir, Rob and Singla, Ankit and Weatherspoon, Hakim},
title = {A {D}ecentralized {SDN} {A}rchitecture for the {W}AN},
year = {2024},
isbn = {9798400706141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3651890.3672257},
doi = {10.1145/3651890.3672257},
abstract = {Motivated by our experiences operating a global WAN, we argue that SDN's reliance on infrastructure external to the data plane has substantially complicated the challenge of maintaining high availability. We propose a new decentralized SDN (dSDN) architecture in which SDN control logic instead runs within routers, eliminating the control plane's reliance on external infrastructure and restoring fate-sharing between control and data planes. We present dSDN as a simpler approach to realizing the benefits of SDN in the WAN. Despite its much simpler design, we show that dSDN is practical from an implementation viewpoint, and outperforms centralized SDN in terms of routing convergence and SLO impact.},
booktitle = {Proceedings of the ACM SIGCOMM 2024 Conference},
pages = {938-953},
numpages = {16},
keywords = {wide-area networks, software-defined networking, traffic engineering},
location = {Sydney, NSW, Australia},
series = {ACM SIGCOMM '24}
}

@inproceedings{10.1145/3102980.3103005,
author = {Huang, Peng and Guo, Chuanxiong and Zhou, Lidong and Lorch, Jacob R. and Dang, Yingnong and Chintalapati, Murali and Yao, Randolph},
title = {Gray Failure: {T}he {A}chilles' {H}eel of {C}loud-{S}cale {S}ystems},
year = {2017},
isbn = {9781450350686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102980.3103005},
doi = {10.1145/3102980.3103005},
abstract = {Cloud scale provides the vast resources necessary to replace failed components, but this is useful only if those failures can be detected. For this reason, the major availability breakdowns and performance anomalies we see in cloud environments tend to be caused by subtle underlying faults, i.e., gray failure rather than fail-stop failure. In this paper, we discuss our experiences with gray failure in production cloud-scale systems to show its broad scope and consequences. We also argue that a key feature of gray failure is differential observability: that the system's failure detectors may not notice problems even when applications are afflicted by them. This realization leads us to believe that, to best deal with them, we should focus on bridging the gap between different components' perceptions of what constitutes failure.},
booktitle = {Proceedings of the 16th Workshop on Hot Topics in Operating Systems},
pages = {150-155},
numpages = {6},
location = {Whistler, BC, Canada},
series = {HotOS '17}
}