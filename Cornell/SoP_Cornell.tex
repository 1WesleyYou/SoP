\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{parskip}

\setlength{\parskip}{8pt}
% Linespread command allows you to change line spacing for the entire document
\linespread{1.18}

% Tweak page margins
\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\textwidth}{1.75in}

\addtolength{\topmargin}{-.875in}
\addtolength{\textheight}{1.75in}

\usepackage{natbib}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{fancyhdr}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\Hrule}{\rule{\linewidth}{0.3mm}}

% Project specific macros
\newcommand{\graphite}{GRAPHITE\xspace}
\newcommand{\wave}{WAVE\xspace}

% School specific macros
\newcommand{\schoolShort}{Cornell\xspace}
\newcommand{\school}{Cornell University\xspace}
\newcommand{\schoolLong}{Cornell University\xspace}

\newcommand{\profOne}{Prof. Ryan Huang\xspace}
\newcommand{\profTwo}{Prof. Xiaonan Huang\xspace}
\newcommand{\profThree}{Prof. Yutong Ban\xspace}

% Creates header for each page
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{\header\hskip\linepagesep\vfootline\thepage}
\newskip\linepagesep \linepagesep 5pt\relax
\def\vfootline{%
    \begingroup
    	\rule[-10pt]{0.75pt}{25pt}
    \endgroup
}
\def\header{%
	\begin{minipage}[]{120pt}
		\hfill Yuchen You
    	\par \hfill 				% Formatting boilerplate
    	CS, PhD, Fall 2026 			% Area, Program, Cycle, Year
    \end{minipage}
}
\fancyhead[RE,LO]{Statement of Purpose | \schoolLong}
\renewcommand\headrulewidth{0pt}

\begin{document}

\textbf{Research Focus and Motivation.}
I aim to build \textbf{reliable distributed systems and ML-serving infrastructure} that connect \textbf{noisy, partial telemetry}---metrics, logs, and anomaly alerts---to \textbf{SLO-aware diagnoses and mitigation actions} that operators can trust in practice. This agenda is shaped by observing \textbf{gray failures}~\cite{10.1145/3102980.3103005} in a ZooKeeper testbed: under injected overload and packet loss, leader election and heartbeats appeared healthy while a subset of clients performing metadata operations suffered multi-second latency spikes and SLO violations long before any built-in alarm triggered. I am especially interested in how \textbf{coordination services (e.g., ZooKeeper)} and \textbf{GPU-based inference stacks} can expose and consume the right signals so that ML and data services remain within their SLOs despite workload shifts and other latent performance pathologies. My recent work on automated mitigation for ZooKeeper overloads and on launch-bound GPU runtimes has given me hands-on experience with \textbf{measurement pipelines} and \textbf{fault-injection testbeds} for \textbf{performance-sensitive runtimes}, and has reinforced my sense that we still lack \textbf{abstractions that connect what operators can observe to the reliability guarantees users expect}.

\textbf{Professional Plans and the Role of a PhD.}
In the long term, I hope to build a career as a \textbf{systems researcher} who designs and evaluates \textbf{operating systems} and \textbf{distributed infrastructure for ML and cloud workloads}. Through my projects on ZooKeeper-based operations, GPU runtimes, and embedded control, I have seen how even sophisticated cloud stacks impose a heavy \textbf{operational burden}: operators still spend substantial effort triaging alerts, localizing performance regressions, and keeping ML and data services within their SLOs. These experiences have convinced me that there is substantial room for principled, research-driven improvements in the \textbf{reliability} of the ML and data platforms that many critical applications depend on. Multi-semester systems projects have also shown me that I enjoy and can sustain the required time, patience, and focus: weeks spent \textbf{instrumenting systems}, chasing down subtle performance anomalies in noisy telemetry, and iteratively refining runtime mechanisms and experiments. What I now lack, and seek from a PhD, is systematic training in how to \textbf{formulate open research questions}, develop \textbf{frameworks that relate concrete mechanisms to reliability guarantees}, and evaluate them under \textbf{realistic workloads and fault scenarios}, so that I can eventually lead research efforts at the intersection of \textbf{distributed systems}, \textbf{networking}, and \textbf{ML infrastructure}.

\textbf{Learning Reliability from Embedded Robotics.}
Before focusing on large-scale systems, I spent several years in \textbf{embedded systems and robotics}, where I learned that failures often stemmed less from the control law than from unreliable data and weak fault handling. As a research assistant in \textbf{UMich's HDRLab} with \textbf{Prof.~Xiaonan~Huang}, I worked on a \textbf{modular robotic arm platform}---work that received a \textbf{best-poster award} at an \textbf{ICRA~2025 workshop}~\cite{you2025origami}---writing low-level firmware and higher-level control code that connected MCUs, sensors, and actuators over field buses and executed model-based dynamics under tight power, bandwidth, and timing constraints. In practice, noisy, delayed, occasionally corrupted sensor streams meant that naïve controllers could quietly push the arm into unsafe regimes. Adding lightweight monitoring and metrics—simple filtering, sanity checks, and degraded-but-safe modes that fall back to redundant signals or conservative trajectories when data quality drops—taught me to view robotic and computing systems in terms of \textbf{observability}, \textbf{fault containment}, and \textbf{graceful degradation} under tight budgets, an outlook I now apply to larger-scale distributed and ML systems.

\textbf{Agentic Operations for ZooKeeper-based Systems.}
In \textbf{UMich's OrderLab} with \textbf{Prof.~Ryan~Huang}, I am exploring how to design \textbf{precision mitigations} for distributed systems: mechanisms that \textbf{minimize SLO violations and downtime} while preserving the \textbf{safety and stability} of the overall cluster. In a ZooKeeper testbed where we injected overload and packet loss, we frequently observed \textbf{gray failures}: leader election and heartbeats appeared healthy, yet a subset of clients experienced multi-second latency spikes and SLO violations before any alarm fired. Rather than building yet another dashboard, we designed an \textbf{agentic operations layer} that treats mitigation as a constrained control problem: a planner periodically reads \textbf{finite-state summaries} and carefully chosen aggregates (e.g., queue backlogs, tail latencies), and selects plans from a \textbf{guard-railed mitigation library} such as rate limiting, selective degradation, or rerouting, instead of issuing arbitrary shell commands. I have led much of the \textbf{measurement pipeline and fault-injection harness}---integrating Prometheus metrics, HAProxy's runtime API, workload generators, and chaos experiments---and used it to study how different observation granularities and action choices shape the trade-off between \textbf{responsiveness} and \textbf{over-reaction}. With noisy, jittery telemetry, we found that there is no single ``optimal'' policy: where to draw the boundary between treating a signal as \textbf{noise}, a \textbf{performance opportunity}, or a \textbf{true failure} is inherently workload- and operator-dependent. This has led me to view mitigation as a space of \textbf{tunable trade-offs}, and to see rich research opportunities in designing \textbf{abstractions and control interfaces} that help operators and agents push this boundary---improving recovery time and availability without sacrificing safety.

\textbf{CUDA Proxy Player: Runtime Support for Launch-Bound Inference.}
In an \textbf{Advanced Operating Systems} course project, I studied bottlenecks in \textbf{launch-bound GPU inference workloads} such as Mixture-of-Experts serving, hypothesizing that in this regime \textbf{host-side orchestration and kernel launches}, rather than raw FLOPs, dominate end-to-end latency. I co-designed \textbf{CUDA Proxy Player}, a multi-path runtime that \textbf{routes requests by size and shape} to eager, \textbf{persistent-kernel}, or \textbf{CUDA Graph} paths, and \textbf{glues these execution flows together} by managing graph capture, replay, and kernel scheduling. I built the benchmarking framework and used it to compare these paths under synthetic traffic, finding that our size-aware routing allows CUDA Graphs to \textbf{outperform a naive eager baseline} in the most launch-bound scenarios while exposing tradeoffs between persistent kernels and co-located high-VRAM jobs. These experiments convinced me that GPU runtimes for ML serving should focus less on a single ``fastest'' path and more on exposing the right \textbf{telemetry and control hooks} for VRAM usage, admission control, and path selection.


%% Non-research accomplishments (e.g. Grades, Academic Service, Work experience) (10-12 lines)

% Grades

% TA and Academic Service

% Industry
\textbf{Fit with Cornell CS, Prof.~Hakim~Weatherspoon, and Prof.~Jiaxin~Lin.}
Cornell’s strengths in systems and networking, and the Computer Systems Laboratory’s joint CS--ECE environment, align closely with my goal of building reliable, accelerator-aware infrastructure for large-scale ML and data-intensive workloads. I see a particularly strong fit with \textbf{Prof.~Hakim~Weatherspoon}, whose recent work in \textbf{Shale}~\cite{10.1145/3651890.3672248} and in \textbf{dSDN}~\cite{10.1145/3651890.3672257} tackles exactly the kind of end-to-end reliability and traffic-engineering challenges I care about. My current project on agentic operations for ZooKeeper/HDFS, which closes the loop from Prometheus-based telemetry to HAProxy- and \texttt{tc}-driven mitigation, has already pushed me to design measurement substrates and guarded control surfaces for coordination services under overload and gray failures; in Prof.~Weatherspoon’s group, I would be excited to extend this line of work to fabric-level mechanisms in reconfigurable DCNs and WANs, studying how congestion control, reconfiguration policies, and control-plane architecture interact with the SLOs of higher-level storage and ML services. At the same time, my earlier experience with cyber-physical reliability in an \textbf{origami-inspired modular soft robotic arm}~\cite{you2025origami}---where limited bandwidth, noisy sensors, and graceful degradation had to be engineered into the control stack---makes me particularly interested in the group’s broader efforts that connect distributed systems techniques to real-world, heterogeneous environments.

I am also strongly drawn to \textbf{Prof.~Jiaxin~Lin}’s research on programmable NICs and systems for ML. Her work on \textbf{RingLeader}~\cite{286477}, which offloads intra-server orchestration to NICs, and on \textbf{Alkali}~\cite{306005}, which provides a portable, high-performance programming framework for SmartNICs, resonates with my interest in co-designing communication, runtimes, and accelerators. In an advanced OS project, I built \emph{CUDA Proxy Player}, a runtime for launch-bound MoE inference that routes requests across eager, persistent-kernel, and CUDA Graph paths based on size and shape, managing kernel orchestration and VRAM usage to reduce end-to-end latency. Combined with my networking background (Mininet router and transport implementations, load-balanced video streaming) and practical experience using HAProxy and \texttt{tc} in distributed systems experiments, this has convinced me that I would enjoy and be prepared for SmartNIC- and interconnect-focused systems work. At Cornell, I would be thrilled to work with Prof.~Weatherspoon and Prof.~Lin---potentially in close collaboration through CSL---on building diagnosable, \textbf{network- and NIC-aware distributed systems} where reliability guarantees are grounded in both the behavior of the underlying fabric and the needs of ML and data-intensive workloads.
%% Summary (3-4 Lines)

% Add some blank space between text and references
% \vspace{0.125in}

% References

% **NOTE**: There are better ways to manage citations in LaTeX, most notably using a bibTeX. I wanted to have greater control on how citations were spaced and formatted and therefore ended up hardcoding them here. Your mileage may wary!

\bibliographystyle{abbrv}
\bibliography{paper}

\end{document}

% That's All Folks.

% Best of luck, you got this! :)