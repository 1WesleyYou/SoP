\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{parskip}

\setlength{\parskip}{8pt}
% Linespread command allows you to change line spacing for the entire document
\linespread{1.18}

% Tweak page margins
\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\textwidth}{1.75in}

\addtolength{\topmargin}{-.875in}
\addtolength{\textheight}{1.75in}

\usepackage{natbib}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{fancyhdr}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\Hrule}{\rule{\linewidth}{0.3mm}}

% Project specific macros
\newcommand{\graphite}{GRAPHITE\xspace}
\newcommand{\wave}{WAVE\xspace}

% School specific macros
\newcommand{\schoolShort}{UMD\xspace}
\newcommand{\school}{University of Maryland\xspace}
\newcommand{\schoolLong}{University of Maryland, College Park\xspace}

\newcommand{\profOne}{Prof. Ryan Huang\xspace}
\newcommand{\profTwo}{Prof. Xiaonan Huang\xspace}
\newcommand{\profThree}{Prof. Yutong Ban\xspace}

% Creates header for each page
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{\header\hskip\linepagesep\vfootline\thepage}
\newskip\linepagesep \linepagesep 5pt\relax
\def\vfootline{%
    \begingroup
    	\rule[-10pt]{0.75pt}{25pt}
    \endgroup
}
\def\header{%
	\begin{minipage}[]{120pt}
		\hfill Yuchen You
    	\par \hfill 				% Formatting boilerplate
    	CS, PhD, Fall 2026 			% Area, Program, Cycle, Year
    \end{minipage}
}
\fancyhead[RE,LO]{Statement of Purpose | \schoolLong}
\renewcommand\headrulewidth{0pt}

\begin{document}

\textbf{Research Focus and Motivation.}
I am applying to the Computer Science PhD program at the University of Maryland, College Park, to work on \textbf{reliable systems} at the intersection of \textbf{distributed systems}, \textbf{networked systems}, and \textbf{ML infrastructure}. I am particularly interested in understanding why complex production systems fail in subtle, non-crash ways---through gray failures~\cite{10.1145/3102980.3103005}, latent performance pathologies, or broken SLOs---and in designing mechanisms that make such systems more \textbf{predictable, observable, and amenable to automatic mitigation}. My current work spans embedded and soft-robotic platforms, distributed coordination services such as ZooKeeper, and GPU-based inference runtimes. Across these projects, I have built instrumentation pipelines, fault-injection and benchmarking frameworks, and performance-sensitive runtimes, which has given me hands-on experience with the kinds of \textbf{measurement, diagnosis, and runtime design} that underlie reliable large-scale systems. This background motivates me to further pursue research on how low-level mechanisms in operating systems, distributed coordination layers, and ML serving stacks can be co-designed to support strong reliability goals.

\textbf{Professional Plans and the Role of a PhD.}
In the long term, I hope to build a career as a \textbf{systems researcher} who designs and evaluates \textbf{operating systems} and \textbf{distributed infrastructure for ML and cloud workloads}. Working with such systems has made me acutely aware of the \textbf{operational burden} they impose: even sophisticated cloud stacks still demand substantial manual effort to triage noisy alerts, localize performance regressions, and keep ML and data services within their SLOs. This has convinced me that we still lack \textbf{principled ways to connect the messy telemetry that operators see to the reliability guarantees that cloud users expect}. My ambition is to help develop \textbf{well-understood, well-evaluated systems} that strengthen the reliability of the ML and data services that many critical societal applications increasingly depend on. Multi-semester systems projects have shown me that I enjoy and can sustain the required time, patience, and focus: weeks spent \textbf{instrumenting systems}, chasing down \textbf{subtle performance anomalies} in noisy telemetry, and iteratively refining runtime mechanisms and experiments. What I now lack, and seek from a PhD, is systematic training in how to \textbf{formulate open research questions}, build \textbf{abstractions that connect mechanisms to reliability guarantees}, and evaluate those abstractions under \textbf{realistic workloads and fault scenarios}, so that I can lead research efforts at the intersection of \textbf{distributed systems, networking, and ML infrastructure}.

\textbf{Learning Reliability from Embedded Robotics.}
Before focusing on large-scale systems, I spent several years in \textbf{embedded systems and robotics}, where I learned that failures often stemmed less from the control law than from unreliable data and weak fault handling. As a research assistant in \textbf{UMich's HDRLab} with \textbf{Prof.~Xiaonan~Huang}, I worked on a \textbf{modular robotic arm platform}, writing low-level firmware and higher-level control code that connected MCUs, sensors, and actuators \textbf{over field buses such as CAN and I\textsuperscript{2}C} and executed model-based dynamics under tight power, bandwidth, and timing constraints. Our project received a \textbf{best-poster award} at an \textbf{ICRA~2025 workshop}~\cite{you2025origami}. In practice, the limiting factor was the data path: noisy, delayed, occasionally corrupted sensor streams meant that naïve controllers could quietly push the arm into unsafe regimes. Adding lightweight monitoring and metrics—simple filtering, sanity checks, and degraded-but-safe modes that fall back to redundant signals or conservative trajectories when data quality drops—made reliability concrete for me and taught me to view robotic and computing systems in terms of \textbf{observability}, \textbf{fault containment}, and \textbf{graceful degradation} under tight budgets, an outlook I now apply to larger-scale distributed and ML systems.

\textbf{Agentic Mitigation for Failures in Distributed Systems.}
I joined \textbf{UMich's OrderLab} with \textbf{Prof.~Ryan Huang} to study how to use \textbf{agentic mechanisms} to automate \textbf{mitigation} for overload and gray failures in ZooKeeper clusters. We treat anomaly detectors and alerts as given inputs and ask a narrower question: \emph{given noisy production-style telemetry and coarse failure signals, can an automated operator choose sequences of mitigation actions that restore SLOs as effectively as, or better than, hand-tuned rule-based controllers?} To study this, I built an end-to-end platform: a feedback controller that ingests \textbf{Prometheus} metrics and alerts, uses \textbf{HAProxy}-based traffic shaping, and exposes a small library of \textbf{unit mitigation actions} (throttling, re-routing, I/O limiting) with explicit parameters and safety constraints. I also implemented a load generator for multi-phase traffic (warmup--spike--steady-state with skewed hotspots), a threshold-based \textbf{static controller} baseline, and a finite-state replay harness that replays ZooKeeper audit logs to reproduce failure trajectories and present a compact \textbf{FSM view of system state} to the operator.

The \textbf{agentic layer} then adds an LLM-based planner whose sole role is \textbf{mitigation planning}: it periodically reads recent metrics, alerts, and FSM summaries, synthesizes short execution plans---sequences of actions from the constrained mitigation library---and hands them to the controller for execution and evaluation. In our experiments, the static controller reliably restores throughput under many overload and crash scenarios, but under evolving gray failures and skewed workloads it often over- or under-reacts, prolonging periods of degraded p95 latency. In those settings, the agent uses the same signals and replayed trajectories to propose more targeted throttling and re-routing plans that shorten SLO-violation windows while staying within predefined safety limits. This project has been my main training ground in distributed systems research: \textbf{framing questions around real failure phenomena}, \textbf{hacking substantial infrastructure}, and \textbf{designing baselines, static scaffolding, and experiments} that turn anecdotal recovery behaviors into evidence about agentic mitigation.

% PROJECT 3: Graphite - Distributed Temporal Graph Processing
\textbf{CUDA Proxy Player: Runtime Support for Launch-Bound Inference.}
In an \textbf{Advanced Operating Systems} course project, I studied bottlenecks in \textbf{launch-bound GPU inference workloads} such as Mixture-of-Experts serving. I co-designed \textbf{CUDA Proxy Player}, a multi-path runtime that \textbf{routes requests by size and shape}: small, latency-sensitive requests follow a \textbf{persistent-kernel} path, while larger, more regular batches are captured into \textbf{CUDA Graphs}. The runtime then \textbf{glues these execution flows together}, managing graph capture, replay, and kernel scheduling so that we can exploit graphs where they help while avoiding their overhead on tiny requests. I built most of the benchmarking framework---microbenchmarks and representative forward-pass graphs across realistic expert sizes and batch shapes---and used it to compare these paths under synthetic traffic. In the launch-bound regime, this size-aware routing and glued execution allowed our CUDA Graph path to \textbf{outperform a naive eager baseline}, reducing end-to-end latency up to roughly \textbf{10\%} in the most launch-bound scenarios.

\paragraph{\textbf{Fit with UMD CS and Prof.~Zaoxing Liu.}}
UMD's strength in \textbf{operating systems}, \textbf{networking}, and \textbf{data-intensive computing}, and its cluster of faculty working on systems measurement and reliability, make it an ideal place for the problems I want to work on. Within this environment, I see a particularly strong fit with \textbf{Prof.~Zaoxing~Liu's} work on \textbf{high-fidelity observability and analytics substrates}. My recent projects---a Prometheus-based mitigation and experimentation loop for ZooKeeper and benchmarking frameworks for GPU inference runtimes---have convinced me that reliable distributed and ML systems often lack \emph{telemetry substrates that expose the right high-fidelity signals at the right cost}. Prof.~Liu’s \textbf{OctoSketch}~\cite{295671} is exactly the kind of real-time, multi-core sketching substrate I wished I had while diagnosing gray failures and backlog-induced SLO violations in my ZooKeeper environment, and its co-design of sketch algorithms with a high-performance runtime aligns with the abstractions I hope to pursue for ML-serving and GPU-centric systems, where fail-slow behavior often first appears as subtle anomalies in kernel-level signals.

I also see natural alignment with \textbf{MeshAgent}~\cite{zhou2025meshagent}, Prof.~Liu’s recent LLM-driven network-operations framework. My own ZooKeeper work explores \textbf{agentic mitigation planning} on a smaller scale, using constrained LLM plans over a typed mitigation library and noisy Prometheus metrics; MeshAgent shows how such agents can be made \textbf{reliable and interpretable} when grounded in structured action spaces and uncertainty-aware telemetry. At UMD, I would be excited to explore how a telemetry substrate like OctoSketch and an agentic control loop in the spirit of MeshAgent could be combined to detect and mitigate \textbf{fail-slow faults} and \textbf{tail-latency anomalies} in distributed coordination services and ML-serving stacks, while engaging with the broader \textbf{systems and networking community} at UMD on questions of measurement, runtime design, and reliability.

\bibliographystyle{abbrv} 
\bibliography{paper} 

\end{document}