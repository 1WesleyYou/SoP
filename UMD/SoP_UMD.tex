\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{parskip}

\setlength{\parskip}{10pt}
% Linespread command allows you to change line spacing for the entire document
\linespread{1.18}

% Tweak page margins
\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\textwidth}{1.75in}

\addtolength{\topmargin}{-.875in}
\addtolength{\textheight}{1.75in}

\usepackage{natbib}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{fancyhdr}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\Hrule}{\rule{\linewidth}{0.3mm}}

% Project specific macros
\newcommand{\graphite}{GRAPHITE\xspace}
\newcommand{\wave}{WAVE\xspace}

% School specific macros
\newcommand{\schoolShort}{UMD\xspace}
\newcommand{\school}{University of Maryland\xspace}
\newcommand{\schoolLong}{University of Maryland, College Park\xspace}

\newcommand{\profOne}{Prof. Ryan Huang\xspace}
\newcommand{\profTwo}{Prof. Xiaonan Huang\xspace}
\newcommand{\profThree}{Prof. Yutong Ban\xspace}

% Creates header for each page
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{\header\hskip\linepagesep\vfootline\thepage}
\newskip\linepagesep \linepagesep 5pt\relax
\def\vfootline{%
    \begingroup
    	\rule[-10pt]{0.75pt}{25pt}
    \endgroup
}
\def\header{%
	\begin{minipage}[]{120pt}
		\hfill Yuchen You
    	\par \hfill 				% Formatting boilerplate
    	CS, PhD, Fall 2026 			% Area, Program, Cycle, Year
    \end{minipage}
}
\fancyhead[RE,LO]{Statement of Purpose | \schoolLong}
\renewcommand\headrulewidth{0pt}

\begin{document}

I am applying to the PhD program in Computer Science at the \school{} to work on reliable systems at the intersection of operating systems, distributed systems, and ML infrastructure, with a particular interest in why complex production systems fail in subtle ways and how we can make them more predictable, diagnosable, and mitigatable.

\textbf{Preparation and Motivation for Graduate Study:}
I am completing a dual-degree in Mechanical Engineering at Shanghai Jiao Tong University and Computer Science at the University of Michigan. Courses in operating systems, distributed systems, networking, security, and robotics and control, together with my TA work, competition teams, and long-term research projects, have given me a solid systems background and shown that I can sustain the patient, independent, and detail-oriented work that graduate research requires.

Before moving fully into systems research, I spent several years working on embedded systems and robotics, which taught me that failures often stem more from unreliable data and weak fault handling than from the control law itself. As an embedded developer for an autonomous sentry robot in \textbf{RoboMaster} competitions and later a research assistant in \textbf{UMich's HDRLab} advised by \profTwo{} on a modular soft robot, I was responsible both for firmware that connected MCUs, sensors, and actuators over CAN and $I^2C$ and for the model-based dynamics and control code that consumed these streams; our HDRLab project received a \textbf{best-poster award} at an \textbf{ICRA 2025 workshop} \cite{you2025origami}. In practice, however, the limiting factor was often not the controller design but the quality and robustness of the data path: sensor readings could be noisy, delayed, or intermittently corrupted. To keep the system safe and usable, I designed a small but explicit metrics pipeline with filtering, sanity checks, and guards, and implemented fallbacks to redundant signals or predefined degraded-but-safe modes when data quality dropped or components failed, while tuning control gains and trajectories to remain stable under these degraded conditions. Working under tight power and timing constraints, I had to design controllers that used limited actuation budget efficiently while still enforcing strong safety and fault-tolerance guarantees. These experiences made the notion of reliability very concrete for me and trained me to think in terms of \textbf{observability, fault containment, and graceful degradation}--an outlook I now apply to larger-scale distributed and ML systems and that ultimately motivated me to study similar questions in software distributed infrastructures.

With this perspective, I joined \textbf{UMich's OrderLab} advised by \profOne{} to work on an ``agentic'' approach to operating ZooKeeper clusters under overload failures and network fluctuations. My primary role has been to design and implement the end-to-end experimental platform: a closed-loop control plane that ingests Prometheus telemetry, uses HAProxy-based traffic shaping, and exposes a library of safe, parameterized mitigation actions (e.g., throttling, re-routing, and I/O limiting). On top of this substrate, I built a \textbf{dedicated load generator} that drives multi-stage traffic lifecycles and a \textbf{static mitigator baseline} that applies simple threshold-based rules, allowing us to quantify recovery behavior in terms of throughput and tail latency. The agentic layer then adds an LLM-based component that periodically summarizes logs and metrics and proposes sequences of mitigations through a narrow, rate-limited interface. In our testbed, the static controller we designed and carefully tuned can already detect and mitigate many overload and crash scenarios quickly, restoring healthy throughput and tail latency in a wide range of cases. However, in more complex and evolving gray-failure settings, the agent is able to use the same signals to synthesize more targeted sequences of mitigation actions with tuned parameters, which shortens the duration of degraded service and improves recovery quality while still respecting the safety guardrails. This project has been my main research training ground in distributed systems, teaching me to reason about failure modes in terms of mitigation effectiveness, to design strong baselines and ablations, and to iteratively refine mechanisms and evaluation methodology with my advisor.

As part of an \textbf{Advanced Operating Systems} course project and a subsequent independent follow-up, I tackled efficiency challenges in serving launch-bounded MoE-style inference workloads. Working with one collaborator, I co-designed the CUDA Proxy Player, a multi-path runtime that can route each request to an eager, CUDA Graph, or persistent-kernel path depending on workload characteristics. Profiling online decoding pipelines revealed dense timelines of sub-100$\mu$s kernels separated by CPU gaps, indicating that the workload was effectively launch-bounded rather than compute-bounded. My main responsibility was to build the benchmarking and evaluation framework: constructing microbenchmarks and compact MoE forward passes that span realistic expert sizes and hidden dimensions, and using them to compare the different execution paths fairly. In this setting, our CUDA Graph implementation matches a hand-tuned fusion baseline while cutting latency by about 24\% versus naive dispatch. Beyond familiarity with CUDA streams, graphs, and persistent kernels, this project taught me how to identify the true bottleneck, design objective experiments, and choose representative real-world cases to highlight the strengths and limits of new runtime mechanisms.

\textbf{Research Interests and Professional Objectives:}
Broadly, I am interested in the design of \textbf{distributed, networked, and ML systems} that remain \textbf{reliable and efficient} at scale. As modern services and ML workloads grow in size and complexity, the underlying storage, coordination, networking, and runtime layers must deliver high throughput and low tail latency despite hardware variability and ever-changing traffic. I am particularly drawn to settings where systems must act on partial, noisy telemetry yet still provide strong end-to-end behavior.

Within this space, my primary research interest is in \textbf{distributed infrastructure for large-scale ML training and inference}, with a focus on making clusters both \textbf{efficient} and \textbf{reliable}. I am interested in how to schedule, place, and coordinate distributed jobs across heterogeneous hardware and networks to improve throughput and tail latency while remaining robust to \textbf{fail-slow} and \textbf{gray failures} that do not manifest as crashes. I would like to design runtime mechanisms and control planes that use system and training signals as sensors for performance risk, and adapt scheduling, checkpointing, and communication so that reliability goals—such as SLOs on throughput and tail latency—are first-class design targets. Long term, I hope to become a systems researcher who designs robust operating systems and distributed infrastructure that cloud operators can rely on. A PhD will allow me to move from course projects to formulating open problems, building principled abstractions, and evaluating them under realistic workloads, preparing me to lead research efforts that strengthen real-world cloud and ML infrastructure.

\textbf{Fit with the University of Maryland Computer Science PhD Program:}
\school's strength in systems, networking, and data-intensive computing aligns closely with my preparation and research interests in reliable operating, distributed, and ML systems. My work on embedded and soft-robot reliability, ZooKeeper-based agentic operations, and CUDA runtimes has trained me to build instrumentation pipelines, fault-injection testbeds, and performance-sensitive runtimes—skills that match well with UMD's focus on making complex networked and data-driven systems observable, diagnosable, and dependable.

I see a particularly strong fit with \textbf{Prof.~Zaoxing~Liu} and the \textbf{FROOT lab}. His research on sketch-based telemetry and approximate analytics for networked and storage systems (e.g., \textbf{UnivMon, NitroSketch and OctoSketch}) and his recent work on \textbf{AI for Network Ops} (e.g., \textbf{MeshAgent} for reliable network management) mirror my own interest in using noisy telemetry streams as sensors for latent performance problems and automating mitigation in complex infrastructures. In my current ``agentic'' ZooKeeper project, I designed a Prometheus-based telemetry pipeline, HAProxy traffic shaping, and an LLM-driven control loop that proposes safe mitigation actions under overload and gray failures. I would be excited to extend these ideas in Prof.~Liu's group by co-designing approximate telemetry, sketch-based analytics, and AI agents that can detect and react to fail-slow behavior and tail-latency anomalies in large-scale ML and storage clusters.

\bibliographystyle{abbrv}
\bibliography{paper}

\end{document}