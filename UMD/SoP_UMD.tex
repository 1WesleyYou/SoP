\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{parskip}

\setlength{\parskip}{10pt}
% Linespread command allows you to change line spacing for the entire document
\linespread{1.18}

% Tweak page margins
\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\textwidth}{1.75in}

\addtolength{\topmargin}{-.875in}
\addtolength{\textheight}{1.75in}

\usepackage{natbib}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{fancyhdr}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\Hrule}{\rule{\linewidth}{0.3mm}}

% Project specific macros
\newcommand{\graphite}{GRAPHITE\xspace}
\newcommand{\wave}{WAVE\xspace}

% School specific macros
\newcommand{\schoolShort}{UMD\xspace}
\newcommand{\school}{University of Maryland\xspace}
\newcommand{\schoolLong}{University of Maryland, College Park\xspace}

\newcommand{\profOne}{Prof. Ryan Huang\xspace}
\newcommand{\profTwo}{Prof. Xiaonan Huang\xspace}
\newcommand{\profThree}{Prof. Yutong Ban\xspace}

% Creates header for each page
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{\header\hskip\linepagesep\vfootline\thepage}
\newskip\linepagesep \linepagesep 5pt\relax
\def\vfootline{%
    \begingroup
    	\rule[-10pt]{0.75pt}{25pt}
    \endgroup
}
\def\header{%
	\begin{minipage}[]{120pt}
		\hfill Yuchen You
    	\par \hfill 				% Formatting boilerplate
    	CS, PhD, Fall 2026 			% Area, Program, Cycle, Year
    \end{minipage}
}
\fancyhead[RE,LO]{Statement of Purpose | \schoolLong}
\renewcommand\headrulewidth{0pt}

\begin{document}

I am applying to the PhD program in Computer Science at the University of Maryland to work on reliable systems at the intersection of operating systems, distributed systems, and ML infrastructure, with a particular interest in why complex production systems fail in subtle ways and how we can make them more predictable, diagnosable, and mitigatable.

\textbf{Preparation and Motivation for Graduate Study:}
I am currently completing a dual-degree program in Mechanical Engineering at Shanghai Jiao Tong University and Computer Science at the University of Michigan. On the CS side, advanced coursework in operating systems, distributed systems, computer networks, and security has given me a solid grounding in systems fundamentals, while robotics, control, and embedded courses from my ME curriculum trained me to think about real-time constraints and safety. Beyond classes, TA work and long-term project experience have convinced me that I enjoy and can sustain the kind of patient, independent, and detail-oriented effort that graduate research requires.

Before moving fully into systems research, I spent several years working on embedded systems and robotics, which taught me that failures often stem more from unreliable data and weak fault handling than from the control law itself. As an embedded developer for an autonomous sentry robot in \textbf{RoboMaster} competitions and later a research assistant in \textbf{UMich's HDRLab} advised by \profTwo{} on a modular soft robot, I was responsible both for firmware that connected MCUs, sensors, and actuators over CAN and $I^2C$ and for the model-based dynamics and control code that consumed these streams; our HDRLab project received a \textbf{best-poster award} at an \textbf{ICRA 2025 workshop} \cite{you2025origami}. In practice, however, the limiting factor was often not the controller design but the quality and robustness of the data path: sensor readings could be noisy, delayed, or intermittently corrupted. To keep the system safe and usable, I designed a small but explicit metrics pipeline with filtering, sanity checks, and guards, and implemented fallbacks to redundant signals or predefined degraded-but-safe modes when data quality dropped or components failed, while tuning control gains and trajectories to remain stable under these degraded conditions. Working under tight power and timing constraints, I had to design controllers that used limited actuation budget efficiently while still enforcing strong safety and fault-tolerance guarantees. These experiences made the notion of reliability very concrete for me and trained me to think in terms of \textbf{observability, fault containment, and graceful degradation}--an outlook I now apply to larger-scale distributed and ML systems and that ultimately motivated me to study similar questions in software distributed infrastructures.

\textbf{Research Experiences and Interests in Reliable Operating, Distributed, and ML Systems:}
I am especially interested in \textbf{fail-slow} and \textbf{gray failures} in \textbf{large-scale ML training and inference}. As clusters scale to thousands of GPUs, subtle performance pathologies can quietly erode throughput and inflate tail latency without ever causing crashes. I would like to design runtime mechanisms and control planes that treat system metrics and training signals as noisy sensors for such latent failures, and adapt scheduling, checkpointing, or collective communication in response, so that clusters can detect and react before problems become visible outages.

More broadly, I am drawn to \textbf{reliability} and \textbf{efficiency} problems across the stack of systems that support \textbf{modern ML workloads}: from distributed storage and coordination services, to communication and scheduling layers for large-scale training, to runtimes and serving systems for dynamic, launch-bounded inference. Across these topics, a unifying theme is building systems that expose the right abstractions to make complex cloud and ML infrastructure more observable, diagnosable, and controllable in production.

With this perspective, I joined \textbf{UMich's OrderLab} advised by \profOne{} to work on an ``agentic'' approach to operating ZooKeeper clusters under overload failures and network fluctuations. My primary role has been to design and implement the end-to-end experimental platform: a closed-loop control plane that ingests Prometheus telemetry, uses HAProxy-based traffic shaping, and exposes a library of safe, parameterized mitigation actions (e.g., throttling, re-routing, and I/O limiting). On top of this substrate, I built a \textbf{dedicated load generator} that drives multi-stage traffic lifecycles and a \textbf{static mitigator baseline} that applies simple threshold-based rules, allowing us to quantify recovery behavior in terms of throughput and tail latency. The agentic layer then adds an LLM-based component that periodically summarizes logs and metrics and proposes sequences of mitigations through a narrow, rate-limited interface. In our testbed, the static controller we designed and carefully tuned can already detect and mitigate many overload and crash scenarios quickly, restoring healthy throughput and tail latency in a wide range of cases. However, in more complex and evolving gray-failure settings, the agent is able to use the same signals to synthesize more targeted sequences of mitigation actions with tuned parameters, which shortens the duration of degraded service and improves recovery quality while still respecting the safety guardrails. This project has been my main research training ground in distributed systems, teaching me to reason about failure modes in terms of mitigation effectiveness, to design strong baselines and ablations, and to iteratively refine mechanisms and evaluation methodology with my advisor.

As part of an \textbf{Advanced Operating Systems} course project and a subsequent independent follow-up, I tackled efficiency challenges in serving launch-bounded MoE-style inference workloads. Working with one collaborator, I co-designed the CUDA Proxy Player, a multi-path runtime that can route each request to an eager, CUDA Graph, or persistent-kernel path depending on workload characteristics. Profiling online decoding pipelines revealed dense timelines of sub-100$\mu$s kernels separated by CPU gaps, indicating that the workload was effectively launch-bounded rather than compute-bounded. My main responsibility was to build the benchmarking and evaluation framework: constructing microbenchmarks and compact MoE forward passes that span realistic expert sizes and hidden dimensions, and using them to compare the different execution paths fairly. In this setting, our CUDA Graph implementation matches a hand-tuned fusion baseline while cutting latency by about 24\% versus naive dispatch. Beyond familiarity with CUDA streams, graphs, and persistent kernels, this project taught me how to identify the true bottleneck, design objective experiments, and choose representative real-world cases to highlight the strengths and limits of new runtime mechanisms.

\textbf{Professional Objectives and the Role of a PhD:}
Long term, I hope to become a systems researcher who designs robust operating systems and distributed infrastructure that cloud operators can rely on. I am motivated by the gap between sophisticated cloud platforms and the fragility we still see in practice--outages, SLO violations, and gray failures--and my realistic goal is to build well-evaluated systems that improve SLO robustness and make such incidents less frequent. A PhD experience will allow me to move beyond course projects to formulating open research questions, building principled abstractions, and evaluating them under realistic workloads, and to learn how to connect low-level mechanisms with high-level reliability goals so that I can eventually lead research efforts that strengthen real-world cloud and ML infrastructure.

\textbf{Fit with the University of Maryland and Prof.~Zaoxing Liu:}
\school's strength in systems, networking, and data-intensive computing aligns closely with my preparation and research interests in reliable operating, distributed, and ML systems. My work on embedded and soft-robot reliability, ZooKeeper-based agentic operations, and CUDA runtimes has trained me to build instrumentation pipelines, fault-injection testbeds, and performance-sensitive runtimesâ€”skills that match well with UMD's focus on making complex networked and data-driven systems observable, diagnosable, and dependable.

I see a particularly strong fit with \textbf{Prof.~Zaoxing~Liu} and the FROOT lab. His research on sketch-based telemetry and approximate analytics for networked and storage systems (e.g., UnivMon, NitroSketch, OctoSketch, DistCache, NetMigrate, and PromSketch) and his recent work on AI for network operations (e.g., MeshAgent for reliable network management) mirror my own interest in using noisy telemetry streams as sensors for latent performance problems and automating mitigation in complex infrastructures. In my current ``agentic'' ZooKeeper project, I designed a Prometheus-based telemetry pipeline, HAProxy traffic shaping, and an LLM-driven control loop that proposes safe mitigation actions under overload and gray failures. I would be excited to extend these ideas in Prof.~Liu's group by co-designing approximate telemetry, sketch-based analytics, and AI agents that can detect and react to fail-slow behavior and tail-latency anomalies in large-scale ML and storage clusters.

More broadly, the systems and ML ecosystem at \school, including research groups in systems for machine learning, distributed and cloud-based storage, and networked systems, provides exactly the environment I am looking for to develop a coherent research agenda on reliable, diagnosable systems for modern cloud and ML workloads. I believe my background in both embedded and large-scale systems, together with my motivation to tackle gray failures in real infrastructures, would allow me to contribute quickly while growing into an independent researcher in Prof.~Liu's lab and the broader UMD community.

\bibliographystyle{abbrv}
\bibliography{paper}

\end{document}