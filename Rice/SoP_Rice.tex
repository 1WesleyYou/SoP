\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{parskip}

\setlength{\parskip}{0pt}
% Linespread command allows you to change line spacing for the entire document
\linespread{1.48}

% Tweak page margins
\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\textwidth}{1.75in}

\addtolength{\topmargin}{-.875in}
\addtolength{\textheight}{1.75in}

\usepackage{natbib}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{fancyhdr}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\Hrule}{\rule{\linewidth}{0.3mm}}

% Project specific macros
\newcommand{\graphite}{GRAPHITE\xspace}
\newcommand{\wave}{WAVE\xspace}

% School specific macros
\newcommand{\schoolShort}{Rice\xspace}
\newcommand{\school}{Rice University\xspace}
\newcommand{\schoolLong}{Rice University\xspace}

\newcommand{\profOne}{Prof. Ryan Huang\xspace}
\newcommand{\profTwo}{Prof. Xiaonan Huang\xspace}
\newcommand{\profThree}{Prof. Yutong Ban\xspace}

% Creates header for each page
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{\header\hskip\linepagesep\vfootline\thepage}
\newskip\linepagesep \linepagesep 5pt\relax
\def\vfootline{%
    \begingroup
    	\rule[-10pt]{0.75pt}{25pt}
    \endgroup
}
\def\header{%
	\begin{minipage}[]{120pt}
		\hfill Yuchen You
    	\par \hfill 				% Formatting boilerplate
    	CS, PhD, Fall 2026 			% Area, Program, Cycle, Year
    \end{minipage}
}
\fancyhead[RE,LO]{Statement of Purpose | \schoolLong}
\renewcommand\headrulewidth{0pt}

\begin{document}

\paragraph{Research Focus and Motivation.}
I am applying to the Computer Science PhD program at \school{} to build \textbf{agentic (LLM-assisted) systems} for \textbf{reliable and efficient AI infrastructure} in cloud deployments of large-scale ML inference.
My research goal is to design \textbf{telemetry-to-action} mechanisms that transform noisy, partial signals (metrics, logs, traces, and alerts) into mitigations that \textbf{reduce tail latency and downtime} while improving \textbf{inference efficiency} (e.g., GPU utilization and cost per token) under real failures and multi-tenancy, and remain \textbf{safe by construction} (e.g., stability and “never-worsen” constraints enforced via constrained action spaces, cooldown/rollback, and offline replay checks).
My recent work on ZooKeeper overload mitigation and launch-bound ML inference runtimes convinced me that today's stacks expose abundant telemetry and knobs, yet often lack a principled interface for \emph{when} and \emph{how} to act---leaving reliability and efficiency dependent on manual interpretation and ad hoc playbooks.

\paragraph{Professional Plans and the Role of a PhD.}
In the long term, I aim to become a \textbf{systems researcher} who designs and evaluates \textbf{reliable and efficient infrastructure for ML inference and cloud workloads}, where correctness, \textbf{tail latency}, and \textbf{cost/throughput efficiency} must hold under failures, load shifts, and multi-tenancy.
I am especially drawn to problems at the boundary between \emph{control} and \emph{systems}: how to turn telemetry into decisions for routing, admission control, and resource management without trading away stability or debuggability.
A PhD is essential training to turn this direction into durable contributions: to formulate sharp research questions, develop abstractions that couple observability with control under explicit safety constraints (e.g., never-worsen envelopes and rollback), and validate them through rigorous experimentation, realistic fault models, and reproducible benchmarks---so improvements in availability and inference performance are systematic rather than dependent on operator heroics.

\paragraph{Reliability Lessons from Embedded Systems.}
Before focusing on large-scale systems, I worked on embedded systems and robotics under \textbf{Prof.~Xiaonan~Huang}; our modular robotic arm project received the \textbf{ICRA 2025 Workshop Best Poster} award~\cite{you2025origami}.
That experience made reliability concrete: failures often stemmed from noisy or delayed signals and brittle fault handling rather than from control laws alone.
For example, I implemented a polling-based \textbf{CAN bus} mechanism to enforce ordered and valid packet reception; when packets were invalid, the controller temporarily fell back to estimated values based on the current motion speed and direction, and if packets were missing for longer windows, it entered a degraded-but-safe mode by capping overall motor output.
This perspective now shapes how I approach distributed and ML systems: prioritize observability, fault containment, and graceful degradation, and then automate mitigation with explicit safety envelopes.

\paragraph{Agentic Distributed System Operations.}
At \textbf{UMich's OrderLab} with \textbf{Prof.~Ryan Huang}, I study safe automated mitigation for overload and gray failures~\cite{10.1145/3102980.3103005} in ZooKeeper under partial observability. Because we act on coarse aggregate telemetry, stability requires trend-based signals and conservative actuation. Our question is whether an agentic controller can \emph{match} a hand-tuned rule-based baseline in routine overload episodes while \emph{retaining higher performance} in complex cases (e.g., skewed workloads and gray failures), without inducing oscillation or unnecessary throttling. To answer this, I built an end-to-end \textbf{ZooKeeper testbed} with Prometheus telemetry, HAProxy traffic shaping, and a constrained library of typed mitigation actions with safety guards, together with a fault-injection and benchmarking harness (\texttt{zkbench}) supporting diverse injected failures and load peaks; we run repeated trials to control variance and exclude an explicit \textbf{warm-up phase} so measurements reflect steady operation. We evaluate consistently using client-side \emph{aggregate throughput}, \emph{average latency}, and \emph{P95 latency} computed over a \textbf{sliding window} to emphasize trends and avoid misclassifying short-term noise as failure behavior. As a strong comparison point, we implemented a \textbf{denoised rule-based baseline} that uses sliding-window trend checks to trigger parameter-tuned mitigations from a playbook; while it prevents collapse, it often over- or under-reacts to compound shifts, degrading P95 latency and throughput. I then built an \textbf{agentic layer} where an LLM-based planner reads the same windowed metrics and finite-state summaries and proposes mitigation plans from the same constrained action library; early iterations exposed action conflicts and over-throttling, motivating a tighter observation/action interface. To keep actuation \textbf{safe} under coarse signals and uncertain state, we treat the LLM as a \textbf{proposal generator} and gate execution with explicit bounds, cooldown, and rollback, along with replay-based checks on recent metric traces. With these guardrails, early runs suggest the agent can remain competitive with the baseline in routine overload episodes while avoiding oscillatory actuation; we are currently expanding the evaluation to quantify throughput, average latency, and P95 latency trends across diverse injected failures and demand shifts. More broadly, this experience convinced me that in reliability control loops, the bottleneck is rarely ``smarter decisions'' but \textbf{designing the right state abstractions and guardrails} so decisions remain stable under noisy signals. Looking ahead, I hope to expand our failure models to stress more complex scenarios and to refine mitigation policies while preserving these guardrails.

\paragraph{CUDA Proxy Player: Runtime Support for Launch-Bound Inference.}
In an Advanced Operating Systems course project, we studied when conditional GPU inference (e.g., Mixture-of-Experts serving) becomes \emph{launch-bound}, where host-side orchestration and frequent kernel launches dominate end-to-end latency.
Motivated by the hypothesis that \emph{shape-stable} segments (graph-safe regions with static shapes/footprints) can be amortized while \emph{unstable} glue (routing/packing/scatter micro-ops) should be isolated, we prototyped CUDA Proxy Player, a multi-path runtime that routes requests to eager execution, CUDA Graph replay, or persistent-kernel execution based on lightweight size/shape stability signals.
My primary contribution was leading the \textbf{micro-benchmarking} and \textbf{proof-of-concept evaluation} that made these trade-offs measurable: I designed controlled experiments that sweep expert sizes, batch shapes, and traffic mixes to quantify where each path wins, and I decomposed end-to-end behavior into true speedups versus \emph{coordination artifacts} (e.g., scheduling and synchronization costs) that can erase kernel-level gains.
Beyond aggregate numbers, I also drove \textbf{root-cause analyses} of counterintuitive anomalies we encountered during evaluation---for example, regimes where larger matrix sizes ran \emph{slower} than smaller ones.
Rather than treating these as noise, I instrumented the runtime and kernels with \textbf{NVIDIA Nsight Systems} markers/traces to localize bottlenecks to specific stages (kernel execution, memory behavior, or orchestration/synchronization), iterating from hypotheses to targeted measurements until the slowdown was explainable.
This experience taught me to treat performance as a scientific question: remain skeptical of apparent trends, diagnose anomalies with direct evidence, and articulate a logically complete chain from observation to root cause.
It also reinforced my view that ML inference runtimes should expose explicit \textbf{telemetry and control hooks}---for launch overhead, VRAM headroom, admission control, and path-selection boundaries---so deployment policies can be tuned to constraints and multi-tenancy rather than chasing a single ``fastest'' path.

\paragraph{Fit with Rice Computer Science and Future Goals.}
Rice's collaborative systems community and rigorous research culture make it an ideal environment for the kind of principled, evidence-driven systems work I hope to pursue.
I am particularly excited to work with \textbf{Prof.~Jiarong~Xing} and \textbf{Prof.~Tony~Geng}, whose systems tackle GenAI serving from complementary angles.
Prof.~Xing's \textbf{SkyWalker}~\cite{xia2025skywalkerlocalityawarecrossregionload} highlights that cross-region routing must preserve KV-cache locality while staying stable under bursty, diurnal shifts; my \textbf{Agentic Distributed System Ops} project built a telemetry-to-action loop for ZooKeeper with \emph{constrained, typed mitigations} and \emph{replay-gated execution} (rollback and offline validation) to avoid oscillation under partial observability, which I am eager to translate into \emph{safety envelopes} for routing and admission control.
Prof.~Geng's \textbf{MGG}~\cite{288647} demonstrates both the power---and the fragility---of fine-grained multi-GPU pipelining and GPU-aware mapping; my \textbf{CUDA Proxy Player} work emphasized separating true speedups from \emph{coordination artifacts} (e.g., launch overheads, synchronization effects) and exposing the right telemetry/control hooks so aggressive pipelining and placement policies remain robust under interference.
At Rice, I hope to study how to make GenAI serving \emph{both adaptive and safe} by (i) designing trend-aware, rollback-gated cross-region routing/admission policies and (ii) developing interference-aware telemetry and controls that keep multi-GPU pipelining decisions stable when contention deviates from ideal profiles.

\bibliographystyle{abbrv}
\bibliography{paper}

\end{document}

% That's All Folks.

% Best of luck, you got this! :)