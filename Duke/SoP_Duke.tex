\documentclass[12pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{setspace}
\usepackage{parskip}

\singlespacing
% \setlength{\parskip}{5pt}
% Linespread command allows you to change line spacing for the entire document
% \linespread{1.18}

% Tweak page margins
\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\textwidth}{1.75in}

\addtolength{\topmargin}{-.875in}
\addtolength{\textheight}{1.75in}

\usepackage{natbib}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{fancyhdr}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\Hrule}{\rule{\linewidth}{0.3mm}}

% Project specific macros
\newcommand{\graphite}{GRAPHITE\xspace}
\newcommand{\wave}{WAVE\xspace}

% School specific macros
\newcommand{\schoolShort}{Duke\xspace}
\newcommand{\school}{Duke University\xspace}
\newcommand{\schoolLong}{Duke University\xspace}

\newcommand{\profOne}{Prof. Ryan Huang\xspace}
\newcommand{\profTwo}{Prof. Xiaonan Huang\xspace}
\newcommand{\profThree}{Prof. Yutong Ban\xspace}

% Creates header for each page
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{\header\hskip\linepagesep\vfootline\thepage}
\newskip\linepagesep \linepagesep 5pt\relax
\def\vfootline{%
    \begingroup
    	\rule[-10pt]{0.75pt}{25pt}
    \endgroup
}
\def\header{%
	\begin{minipage}[]{120pt}
		\hfill Yuchen You
    	\par \hfill 				% Formatting boilerplate
    	CS, PhD, Fall 2026 			% Area, Program, Cycle, Year
    \end{minipage}
}
\fancyhead[RE,LO]{Statement of Purpose | \schoolLong}
\renewcommand\headrulewidth{0pt}

\begin{document}

\textbf{Research Objectives.}
My long-term goal is to help build \textbf{reliable and efficient infrastructure for large-scale machine learning and latency-sensitive services}. As models scale to thousands of accelerators and complex multi-tenant clusters, \textbf{gray failures} and performance anomalies become the norm rather than the exception. Unlike clean crash failures, these ``fail-slow'' behaviors quietly erode throughput and fairness. I am particularly interested in systems that can \textbf{detect and respond to such problems in real time}, using telemetry to drive adaptive runtime and scheduling decisions. This agenda is the main reason I am applying to the \textbf{Ph.D.\ program in Computer Science at Duke}.

\paragraph{Professional Plans and the Role of a PhD.}
In the long term, I aim to become a \textbf{systems researcher} who designs and evaluates \textbf{reliable infrastructure for ML and cloud workloads}, where correctness and latency behavior matter under real failures and multi-tenancy. A PhD is essential training to turn this direction into durable contributions: to formulate sharp research questions, develop abstractions that couple observability with control under explicit safety constraints, and validate them through rigorous experimentation and realistic fault models---so reliability improves systematically rather than by operator heroics.

\paragraph{Reliability Lessons from Embedded Systems.}
Before focusing on large-scale systems, advised by \textbf{Prof.~Xiaonan~Huang}, I worked on embedded and robotics; our modular robotic arm project was selected for a \textbf{Best Poster Spotlight} at an \textbf{ICRA workshop}. That experience made reliability concrete: failures often came from noisy or delayed signals and weak fault handling rather than from control laws alone. Simple monitors, sanity checks, and degraded-but-safe modes were often the difference between a system that merely worked in ideal conditions and one that remained usable under stress. This perspective now shapes how I approach distributed and ML systems: prioritize observability, fault containment, and graceful degradation.

\paragraph{Agentic Distributed System Operations.}
At \textbf{UMich’s OrderLab} with \textbf{Prof.~Ryan Huang}, I study safe automated mitigation for overload and gray failures in ZooKeeper under partial observability. We assume coarse anomaly signals under partial observability. Our question is whether an agentic controller can match a hand-tuned rule-based baseline on \emph{aggregate client throughput} and \emph{avg/p95 latency} without inducing oscillation or throttling. To answer this, I built an end-to-end \textbf{ZooKeeper testbed} with Prometheus telemetry, HAProxy traffic shaping, and a constrained library of typed mitigation actions with safety guards, together with a fault-injection and benchmarking harness (\texttt{zkbench}) that supports weighted injections and load-peak scenarios; we run configurations repeatedly to control variance. As a strong baseline, we implemented a \textbf{denoised rule-based baseline} that uses \textbf{sliding-window trend checks} to trigger mitigations and executes parameter-tuned mitigations from a YAML playbook mapping failure modes to actions. While this baseline prevents collapse, it often over- or under-reacts to workload shifts, leaving extended tail-latency windows. I then built an \textbf{agentic layer} where an LLM-based planner reads metrics and finite-state summaries and proposes detailed plans from the same constrained mitigation library; early failures under a richer API (conflicting actions and over-throttling) pushed me to tighten observation/action interfaces. To keep actuation \textbf{safe} under coarse signals and uncertain system state, we treat the LLM as a \textbf{proposal generator} and gate execution with safety bounds and replay-based validation on recent traces. With these guardrails, the agent consistently shortened SLO-violation windows under gray failures and skewed workloads. More broadly, this experience convinced me that in reliability control loops, the bottleneck is rarely ``smarter decisions'' but \textbf{designing the right state abstractions and guardrails} so decisions remain stable under noisy signals. Looking ahead, I hope to expand our failure models to stress more complex scenarios and to improve performance by refining mitigation policies while preserving these guardrails.

\paragraph{CUDA Proxy Player: Runtime Support for Launch-Bound Inference.}
In an Advanced Operating Systems course project, we asked when conditional GPU inference (e.g., Mixture-of-Experts serving) truly becomes \emph{launch-bound}, with host-side orchestration dominating end-to-end latency. Based on the hypothesis that \emph{shape-stable} regions can be amortized while \emph{unstable} glue should be isolated, I co-designed CUDA Proxy Player: a multi-path runtime that routes requests by size and shape to eager, CUDA Graph replay, or persistent-kernel execution, and stitches these flows together via explicit capture/replay and scheduling interfaces. I built the benchmarking framework---microbenchmarks and end-to-end experiments spanning expert sizes, batch shapes, and traffic mixes---to quantify where each path wins and why. In the most launch-bound regimes, size-aware routing enabled the CUDA Graph path to deliver substantial gains over a naïve eager baseline; however, our breakdown also showed why a hybrid graph+worker design often fails to translate into end-to-end wins: coordination overheads (worker scheduling and phase barriers) can dominate, and keeping workers/buffers resident reduces VRAM headroom and can worsen tail latency under multi-tenant workloads. Designing these experiments taught me to separate real speedups from coordination artifacts, and reinforced my view that ML serving runtimes should expose telemetry and control hooks—for VRAM usage, admission control, and path selection---so operators can tune policies to deployment constraints rather than chasing a single ``fastest'' path.

\textbf{Fit with Duke.} Duke's systems community is an ideal setting for me to pursue reliable AI infrastructure through measurement-driven design. I see strong alignment with \textbf{Prof. Danyang Zhuo}; for his recent work on efficient inference (\textit{FlashSVD}) and simulation-based estimation (\textit{Phantora}), my experience building the \textbf{CUDA Proxy Player} runtime positions me to help design serving systems that explicitly manage coordination overheads and VRAM fragmentation. I am particularly eager to extend his research on hardware-assisted performance isolation (\textit{Harmonic}) by applying my work on safe agentic mitigation. Complementing this, I find deep resonance with \textbf{Prof. Xiaowei Yang}'s data-driven approach to reliability. Her work on proactive maintenance using noisy physical-layer signals (\textit{CableMon}) parallels my goal of formulating ``telemetry-to-action'' loops under partial observability. I aim to synthesize these perspectives---combining Prof. Zhuo's high-velocity infrastructure with Prof. Yang's rigorous measurement-based abstractions---to build self-stabilizing control planes for next-generation cloud and AI datacenters.

\end{document}