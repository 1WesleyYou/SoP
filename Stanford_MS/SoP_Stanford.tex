\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{setspace}
\usepackage{parskip}

\singlespacing
\setlength{\parskip}{6pt}
% Linespread command allows you to change line spacing for the entire document
% \linespread{1.18}

% Tweak page margins
\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\textwidth}{1.75in}

\addtolength{\topmargin}{-.875in}
\addtolength{\textheight}{1.75in}

\usepackage{natbib}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{fancyhdr}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\Hrule}{\rule{\linewidth}{0.3mm}}

% Project specific macros
\newcommand{\graphite}{GRAPHITE\xspace}
\newcommand{\wave}{WAVE\xspace}

% School specific macros
\newcommand{\schoolShort}{Stanford\xspace}
\newcommand{\school}{Stanford University\xspace}
\newcommand{\schoolLong}{Stanford University MSCS\xspace}

\newcommand{\profOne}{Prof. Ryan Huang\xspace}
\newcommand{\profTwo}{Prof. Xiaonan Huang\xspace}
\newcommand{\profThree}{Prof. Yutong Ban\xspace}

% Creates header for each page
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{\header\hskip\linepagesep\vfootline\thepage}
\newskip\linepagesep \linepagesep 5pt\relax
\def\vfootline{%
    \begingroup
    	\rule[-10pt]{0.75pt}{25pt}
    \endgroup
}
\def\header{%
	\begin{minipage}[]{120pt}
		\hfill Yuchen You
    	\par \hfill 				% Formatting boilerplate
    	MSCS, Fall 2026 			% Area, Program, Cycle, Year
    \end{minipage}
}
\fancyhead[RE,LO]{Statement of Purpose | \schoolLong}
\renewcommand\headrulewidth{0pt}

\begin{document}

\textbf{Motivation and Goals.}
I am applying to the Stanford MS in Computer Science (MSCS) to deepen my training in \textbf{computer systems and networking} in an environment where \textbf{systems and AI} are developed side by side. Coming from a dual degree in mechanical engineering and computer science, I have seen how fragile large-scale ML and data services can be in practice: small configuration errors, gray failures, and skewed workloads can all break otherwise well-designed systems. Staying only on my current undergraduate research path would keep me focused on a single stack and lab; I now feel a need for broader, graduate-level training that exposes me to different abstractions, workloads, and design philosophies than I can access at Michigan alone. Rather than going directly into industry or committing immediately to a PhD, I want a \textbf{research-oriented master's} that lets me deepen my systems foundations, explore adjacent ML and networking topics, and decide---with more mature experience---whether my next step should be \textbf{industry research and development} or \textbf{later doctoral study}.

I see Stanford as the right place for that transition: the MSCS combines rigorous systems training with close proximity to cutting-edge ML workloads, and its emphasis on substantial course projects would let me test ideas about \textbf{reliability}, \textbf{observability}, and \textbf{performance} on realistic platforms rather than only on small class assignments. In the long term, I hope to work as a \textbf{systems engineer and applied researcher} designing operating systems and distributed services that remain dependable as they scale; the MSCS is, for me, a deliberate step toward that goal.

\textbf{Academic Preparation and Collaboration.}
As a dual-degree student in \textbf{Mechanical Engineering} at Shanghai Jiao Tong University and \textbf{Computer Science} at the University of Michigan, I have been trained to think about systems from both the abstractions down and the hardware up. Courses in data structures and algorithms and core \textbf{systems and networking} classes such as \textbf{operating systems} and \textbf{computer networks}, together with intensive labs and project-based offerings, have required me to reason carefully about \textbf{performance, correctness, and scalability} while writing substantial amounts of code. In these courses, I repeatedly worked on multi-thousand-line systems with open-ended specifications, strict performance targets, and little scaffolding, which have forced me to design and debug complex behaviors under pressure and have given me confidence that I can \textbf{thrive in rigorous, project-based graduate courses} where much of the learning happens through demanding implementations rather than exams alone.

In team projects, I have led small-group design discussions, helped classmates debug subtle \textbf{concurrency and networking bugs}, and coordinated milestones under tight deadlines. These experiences taught me to \textbf{communicate technical trade-offs clearly}, to support teammates with very different levels of experience, and to take responsibility for both high-level design and engineering-quality details. At Stanford, I hope to contribute by bringing this experience to \textbf{systems course projects}, \textbf{reading groups}, and lab collaborations, and by helping to build inclusive teams where quieter students also have space to shape designs.

\textbf{Embedded and Soft Robotics Foundations.}
My earliest concrete exposure to \textbf{reliability} came from \textbf{embedded systems} and \textbf{soft robotics}. On a robotics team at SJTU, I first worked on autonomous sentry robots with tightly constrained real-time control. Later, at the \textbf{HDR Lab} at UMich with \textbf{Prof.~Xiaonan Huang}, I helped develop an \emph{origami-inspired soft robotic arm}, leading parts of the control stack and experimental setup for a modular, compliant manipulator; this work led to a \textbf{spotlight talk} and \textbf{best-poster} at an \textbf{ICRA 2025 workshop}~\cite{you2025origami}. These systems almost never failed by simply crashing. Instead, we saw gradual \textbf{drift}, \textbf{noisy or saturated sensors}, and partial actuator or communication faults that could push the arm into unstable or unsafe regimes long before any obvious error flag appeared. To keep the robots usable, I helped design lightweight telemetry and health checks, along with degraded-but-safe fallback modes that contained faults and kept behavior predictable under uncertainty. Thinking in terms of \textbf{observability}, \textbf{fault containment}, and \textbf{graceful degradation} pushed me to look beyond individual controllers toward the larger systems that couple software, hardware, and the physical world, and later shaped how I reason about similar issues in \textbf{distributed services} and \textbf{large-scale computing platforms}.

\textbf{Systems Research and Large-Scale Infrastructure.}
With this mindset, I joined \textbf{UMich's OrderLab}, advised by \textbf{Prof.~Ryan Huang}, to study how to keep \textbf{ZooKeeper} clusters stable under \textbf{overload} and \textbf{fluctuating network conditions}. My primary contribution is an end-to-end experimental platform: a \textbf{closed-loop controller} that consumes cluster metrics from standard monitoring tools (such as \textbf{Prometheus}), uses a proxy layer for \textbf{traffic shaping}, and exposes a small \textbf{library of safe, parameterized mitigation actions}---throttling, re-routing, and I/O limiting---that can be invoked automatically. On top of this substrate, I built a \textbf{load generator} that drives multi-phase traffic patterns (warmup, spike, and steady-state) and introduces skewed per-node request rates to emulate real hotspots and imbalances, along with a carefully tuned \textbf{static controller} that triggers mitigations based on simple thresholds. This baseline lets us quantify recovery behavior by tracking throughput and \textbf{p95 latency} across traffic phases. An \textbf{agentic layer} then adds an \textbf{LLM-based component} that periodically summarizes recent logs and metrics into a short execution plan---a sequence of mitigation actions with concrete parameters drawn from the library---which the controller executes and evaluates. In our testbed, the static controller already handles many overload and crash scenarios; in more subtle, evolving \textbf{gray failures}, where the system is ``up'' but degraded, the agent uses the same signals to discover more targeted mitigation plans that shorten periods of poor performance while respecting predefined safety limits. Working on this project has been my main training in \textbf{distributed-systems research}: reasoning about failure modes in terms of mitigation effectiveness, designing strong baselines and ablations, and iteratively refining mechanisms and evaluation methodology with my advisor.

In parallel, I explored \textbf{GPU runtime support for ML inference} in an \textbf{Advanced Operating Systems} course project that grew into an independent follow-up. Working with two collaborators, I co-designed \textbf{CUDA Proxy Player}, a small runtime for \textbf{launch-bounded inference workloads}---situations where many tiny GPU kernels make \textbf{kernel launches and CPU coordination} more expensive than the math itself. The runtime can route each request to one of several execution modes (a straightforward eager launch, a batched launch using \textbf{CUDA Graphs}, or a long-running \textbf{persistent kernel}) depending on workload characteristics. To understand when each mode wins, I implemented most of the benchmarking and evaluation framework, constructing microbenchmarks and compact forward-pass graphs that span realistic model sizes and batch shapes and exercising them under controlled traffic patterns. In these launch-bounded regimes, our CUDA Graph path matches a hand-tuned fusion baseline while cutting end-to-end latency by about \textbf{24\%} relative to a naive eager baseline. This project reinforced my view of performance as a \textbf{runtime design problem}: identifying the true bottleneck, articulating clear baselines, and evaluating design choices on representative workloads.

\textbf{Research Interests and Stanford Fit.}
Building on these projects, I am especially interested in \textbf{reliable systems} at the intersection of \textbf{operating systems}, \textbf{distributed systems}, and \textbf{ML/data infrastructure}, with a focus on why complex production platforms often fail in \textbf{subtle, gray ways} and how we can make them more \textbf{predictable, observable, and mitigatable}. Across the embedded, distributed, and GPU settings I have worked in, a unifying theme is designing \textbf{reliability-oriented abstractions}---telemetry and control interfaces that make it easier to explain SLO violations, contain failures, and support safer, partially automated mitigation in production.

At Stanford, I hope to turn these interests into a more \textbf{principled and broadly applicable} skill set. I am especially drawn to combining advanced systems and networking offerings with project-based courses such as \textbf{CS 329S (Machine Learning Systems Design)}, which emphasizes deploying and monitoring real-world ML systems, and \textbf{CS 336 (Language Modeling from Scratch)}, which would deepen my understanding of large language models as demanding, resource-intensive workloads. Beyond individual classes, I am excited by \textbf{Stanford's systems and ML ecosystem}---reading groups, research seminars, and cross-lab collaborations that would expose me to different perspectives on reliability from networking, storage, and ML researchers.

I would be eager to learn from \textbf{systems and ML faculty} such as \textbf{Prof.~Christopher R\'e}, whose work on machine learning and data systems examines how software and hardware platforms evolve under ML workloads, and \textbf{Prof.~Keith Winstein}, whose group designs new networked systems by rethinking abstractions around communication, compression, and performance; both perspectives align closely with my interest in reliable infrastructure for ML and data-intensive services. Looking ahead, I aim to \textbf{build and study large-scale infrastructure}---either in an industrial research lab or through further graduate study after the MS---and I see the Stanford MSCS as a chance both to sharpen my technical foundations and to contribute to a community where systems and AI are advanced together.

\bibliographystyle{abbrv}
\bibliography{paper}

\end{document}

% That's All Folks.

% Best of luck, you got this! :)