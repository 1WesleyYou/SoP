\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{parskip}
\usepackage{setspace}
\doublespacing

\setlength{\parskip}{2pt}
% Linespread command allows you to change line spacing for the entire document
% \linespread{1.18}

% Tweak page margins
\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\textwidth}{1.75in}

\addtolength{\topmargin}{-.875in}
\addtolength{\textheight}{1.75in}

\usepackage{natbib}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{fancyhdr}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\Hrule}{\rule{\linewidth}{0.3mm}}

% Project specific macros
\newcommand{\graphite}{GRAPHITE\xspace}
\newcommand{\wave}{WAVE\xspace}

% School specific macros
\newcommand{\schoolShort}{NYU\xspace}
\newcommand{\school}{New York University\xspace}
\newcommand{\schoolLong}{New York University\xspace}

\newcommand{\profOne}{Prof. Ryan Huang\xspace}
\newcommand{\profTwo}{Prof. Xiaonan Huang\xspace}
\newcommand{\profThree}{Prof. Yutong Ban\xspace}

% Creates header for each page
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{\header\hskip\linepagesep\vfootline\thepage}
\newskip\linepagesep \linepagesep 5pt\relax
\def\vfootline{%
    \begingroup
    	\rule[-10pt]{0.75pt}{25pt}
    \endgroup
}
\def\header{%
	\begin{minipage}[]{120pt}
		\hfill Yuchen You
    	\par \hfill 				% Formatting boilerplate
    	CS, PhD, Fall 2026 			% Area, Program, Cycle, Year
    \end{minipage}
}
\fancyhead[RE,LO]{Statement of Purpose | \schoolLong}
\renewcommand\headrulewidth{0pt}

\begin{document}

\textbf{Research Focus and Motivation.}
I am applying to the Computer Science PhD program at \school{} to work on reliable distributed systems and ML-serving infrastructure. Concretely, I want to understand how to turn noisy, partial telemetry---metrics, logs, and anomaly alerts---from coordination services and GPU inference stacks into SLO-aware diagnoses and mitigation actions that operators can trust. Recent projects on automated mitigation for ZooKeeper overloads and on launch-bound GPU runtimes have given me hands-on experience building measurement pipelines, fault-injection testbeds, and performance-sensitive runtimes, and have convinced me that we still lack abstractions that connect what operators can observe to the reliability guarantees users expect. Advanced systems courses and paper readings on distributed storage, fault tolerance, and ML infrastructure have sharpened this view, showing both how much progress recent work has made and how difficult it remains to reason about reliability in the presence of gray failures~\cite{10.1145/3102980.3103005} and shifting workloads. Earlier, work on embedded and soft robotics---writing firmware and control code for a modular robotic arm under tight power, bandwidth, and timing constraints, and co-presenting our results as a best-poster spotlight at an ICRA workshop~\cite{you2025origami}---taught me that failures often stem from noisy or delayed data and weak fault handling rather than from control laws alone. Adding simple monitoring, sanity checks, and degraded-but-safe modes made reliability concrete for me and shaped how I now think about distributed and ML systems: in terms of observability, fault containment, and graceful degradation.

\textbf{Agentic Distributed System Operations.}
At UMich’s OrderLab with Prof.~Ryan Huang, I study how to use agentic mechanisms to automate mitigation for overload and gray failures in ZooKeeper clusters. We treat anomaly detectors as given and ask a narrower question: given noisy telemetry and coarse failure signals, can an automated operator choose mitigation plans that are as effective and safe as hand-tuned controllers? To explore this, I helped build an end-to-end platform around a ZooKeeper cluster with Prometheus metrics and alerts, HAProxy-based traffic shaping, a small library of typed mitigation actions with safety guards, synthetic workloads, and a threshold-based controller baseline. Experiments showed that this baseline prevents collapse but often over- or under-reacts to workload changes, leaving extended windows of elevated tail latency. I then worked on an agentic layer where an LLM-based planner reads metrics and finite-state summaries and proposes short plans composed from the constrained mitigation library. Early failures---when a richer API let the planner oscillate between conflicting actions and over-throttle healthy traffic---taught me the importance of tight observation and action interfaces. After constraining and validating plans via replay, the agent consistently shortened SLO-violation windows under gray failures and skewed workloads. Looking ahead, I hope to extend this platform with richer detectors and more adaptive policies while preserving these guard rails, so agentic controllers can be trusted in more realistic multi-tenant, multi-service deployments.

\textbf{CUDA Proxy Player: Runtime Support for Launch-Bound Inference.}
In an Advanced Operating Systems project, I studied bottlenecks in launch-bound GPU inference workloads such as Mixture-of-Experts serving, where host-side orchestration and kernel launches can dominate end-to-end latency. I co-designed CUDA Proxy Player, a multi-path runtime that routes requests by size and shape to eager, persistent-kernel, or CUDA Graph paths. The runtime glues these execution flows together, managing graph capture, replay, and kernel scheduling so that we can exploit graphs where they help while avoiding their overhead on tiny requests. I built the benchmarking framework---microbenchmarks and end-to-end experiments over varying expert sizes, batch shapes, and traffic mixes---and used it to compare these paths under realistic loads. In the most launch-bound scenarios, size-aware routing and glued execution allowed the CUDA Graph path to outperform a naïve eager baseline, reducing latency by 10\%. However, keeping persistent workers and buffers resident in GPU memory reduced headroom for other high-VRAM jobs and sometimes worsened tail latency under multi-tenant workloads. Designing these experiments, especially separating algorithmic speedups from measurement artifacts, taught me to be careful about what is being optimized and measured. Together, these tradeoffs convinced me that GPU runtimes for ML serving should focus less on a fastest path and more on exposing telemetry and control hooks for VRAM usage, admission control, and path selection, so that operators can tune policies to workload and deployment constraints.

\textbf{Fit with \schoolShort{} GSAS Computer Science and Future Goals.}
\school{}'s Graduate School of Arts and Science and its Computer Science department, with strengths in systems, networking, and ML systems, align with my goal of building reliable infrastructure for large-scale ML workloads and with \schoolShort{}'s culture of combining rigorous systems building with principled reasoning about correctness, performance, and fairness. I see particularly strong alignment with \textbf{Prof.~Aurojit~Panda}, whose recent work connects distributed systems reliability with large-scale ML workloads. In \textbf{Ellsberg}~\cite{306045}, he develops runtime monitors that compare real-world protocol implementations against abstract specifications to detect safety violations in production; and in \textbf{CLM}~\cite{zhao2025clmremovinggpumemory}, he co-designs a system that offloads Gaussians to CPU memory and pipelines CPU--GPU transfers to overcome GPU memory limits in 3DGS workloads. These projects mirror the challenges I face when keeping ZooKeeper control planes reliable under agentic mitigation and managing GPU memory and tail latency in launch-bound inference runtimes. I would also be eager to work with \textbf{Prof.~Jinyang~Li} on problems at the intersection of distributed systems and ML training and serving; his systems such as \textbf{Understanding Stragglers in Large Model Training Using What-if Analysis}~\cite{lin2025understandingstragglerslargemodel} and \textbf{Stateful Large Language Model Serving with Pensiev}~\cite{Yu_2025} address the reliability and resource-management issues I encounter in my ZooKeeper fault-mitigation and launch-bound GPU runtime projects. My experience designing an agentic control plane for ZooKeeper and a launch-bound GPU runtime prepares me to contribute to this ecosystem by building measurement-driven runtimes and control planes that expose the right signals and knobs to operators. Looking ahead, I hope to develop a research agenda on systems for efficient and reliable large-scale ML and, ultimately, to pursue a research-intensive career---ideally as a faculty member---designing and evaluating platforms that make large-model training and serving more predictable and robust for practitioners.

\bibliographystyle{abbrv}
\bibliography{paper}

\end{document}