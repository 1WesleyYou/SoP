\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{parskip}
\usepackage{setspace}
\doublespacing

\setlength{\parskip}{2pt}
% Linespread command allows you to change line spacing for the entire document
% \linespread{1.18}

% Tweak page margins
\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\textwidth}{1.75in}

\addtolength{\topmargin}{-.875in}
\addtolength{\textheight}{1.75in}

\usepackage{natbib}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{fancyhdr}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\Hrule}{\rule{\linewidth}{0.3mm}}

% Project specific macros
\newcommand{\graphite}{GRAPHITE\xspace}
\newcommand{\wave}{WAVE\xspace}

% School specific macros
\newcommand{\schoolShort}{NYU\xspace}
\newcommand{\school}{New York University\xspace}
\newcommand{\schoolLong}{New York University\xspace}

\newcommand{\profOne}{Prof. Ryan Huang\xspace}
\newcommand{\profTwo}{Prof. Xiaonan Huang\xspace}
\newcommand{\profThree}{Prof. Yutong Ban\xspace}

% Creates header for each page
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{\header\hskip\linepagesep\vfootline\thepage}
\newskip\linepagesep \linepagesep 5pt\relax
\def\vfootline{%
    \begingroup
    	\rule[-10pt]{0.75pt}{25pt}
    \endgroup
}
\def\header{%
	\begin{minipage}[]{120pt}
		\hfill Yuchen You
    	\par \hfill 				% Formatting boilerplate
    	CS, PhD, Fall 2026 			% Area, Program, Cycle, Year
    \end{minipage}
}
\fancyhead[RE,LO]{Statement of Purpose | \schoolLong}
\renewcommand\headrulewidth{0pt}

\begin{document}

I am applying to the PhD program in Computer Science at \schoolShort{} to study systems for \textbf{efficient} and \textbf{reliable large-scale machine learning}, \textbf{spanning operating systems}, \textbf{distributed systems}, and \textbf{ML infrastructure}. My central goal is to design training and serving platforms that achieve high throughput and low latency while remaining robust to fail-slow hardware, and other performance pathologies in modern clusters. 

\textbf{Early Embedded and Robotics Experience.}
My path into systems reliability began in embedded systems and soft robotics. As a control developer for an autonomous sentry robot in RoboMaster competitions and later a research assistant in UMich's HDRLab advised by \textbf{Prof.\ Xiaonan Huang} on a modular soft robotic arm, I worked from low-level firmware up to model-based dynamics and control code; our project received a \textbf{best-poster award} at an \textbf{ICRA 2025 workshop}\cite{you2025origami}. In practice, failures often came from the data path: sensor readings that were noisy, delayed, or intermittently corrupted. To keep the system safe, I built a small metrics pipeline with filtering and sanity checks, and implemented fallbacks to redundant signals or predefined degraded-but-safe modes when data quality dropped. Working under tight power and timing constraints forced me to design controllers that used limited actuation budget efficiently while still enforcing strong safety guarantees. These experiences made reliability very concrete for me and trained me to think in terms of observability, fault containment, and graceful degradation—an outlook I now apply to larger-scale distributed and ML systems.

\textbf{Agentic Operations for Failures in Distributed Systems.}
With this perspective, I joined UMich's OrderLab advised by \textbf{Prof.\ Ryan Huang} to work on an ``agentic'' approach to operating ZooKeeper clusters under overload and network fluctuations. My primary role has been to design and implement the end-to-end experimental platform: a closed-loop control plane that ingests Prometheus telemetry, uses HAProxy-based traffic shaping, and exposes a library of safe, parameterized mitigation actions such as throttling, re-routing, and I/O limiting. On top of this substrate, I built a dedicated load generator and a static mitigator baseline so we can quantify recovery behavior in terms of throughput and tail latency. The agentic layer then adds an LLM-based component that periodically summarizes logs and metrics and proposes sequences of mitigations through a narrow, rate-limited interface. In our testbed, the static controller can already detect and mitigate many overload and crash scenarios quickly, restoring healthy throughput and tail latency. However, in more complex and evolving gray-failure settings, the agent uses the same signals to synthesize more targeted mitigation sequences with tuned parameters, shortening the duration of degraded service while respecting safety guardrails. This project has been my main research training ground in distributed systems, teaching me to reason about failure modes in terms of mitigation effectiveness and to iteratively refine mechanisms and evaluation methodology.

\textbf{ML Systems and CUDA Runtime Work.}
To broaden my perspective on ML systems, I led the benchmarking framework for a ``CUDA Proxy Player'' runtime as part of an Advanced Operating Systems course project and a subsequent independent follow-up. Working with one collaborator, I co-designed a multi-path runtime that routes each request to an eager, CUDA Graph, or persistent-kernel path depending on workload characteristics. Profiling online decoding pipelines revealed dense timelines of sub-100$\mu$s kernels separated by CPU gaps, showing that the workload was effectively launch-bounded rather than compute-bounded. I built the benchmarking and evaluation framework, constructing microbenchmarks and compact MoE forward passes to compare these execution paths fairly. In this setting, our CUDA Graph implementation matches a hand-tuned fusion baseline while cutting latency by about 24\% versus naive dispatch. This project deepened my familiarity with CUDA streams, graphs, and persistent kernels, and taught me how to identify true bottlenecks and design objective experiments for new runtime mechanisms.

\textbf{Fit with \schoolShort{} GSAS Computer Science.}
\school{}'s Graduate School of Arts and Science is an excellent place for me to pursue these interests. The Computer Science department's strengths in systems, networking, and security match my goal of building reliable infrastructure for ML workloads. I am particularly drawn to \schoolShort{}'s culture of combining rigorous systems building with principled reasoning about correctness, performance, and fairness—from networked and storage systems to large-scale ML training platforms. I would be excited to contribute to this community and to collaborate with students and faculty working across systems, networking, and machine learning.
I see strong alignment with \textbf{Prof.\ Aurojit Panda}, whose work on reliable networked systems—from verifying distributed protocols and dataplanes to analyzing stragglers in large-model training—directly connects to the faults I study in ZooKeeper using telemetry, fault injection, and agentic mitigation. I am also keen to work with \textbf{Prof.\ Jinyang Li}, whose research in machine learning systems, distributed and parallel computing, and databases builds abstractions and runtimes for scalable training and serving, as in her recent projects on distributed 3D Gaussian Splatting, out-of-core GNN and LLM training, and stateful LLM serving. My experience designing an agentic control plane for ZooKeeper and a CUDA runtime for launch-bounded inference prepares me to contribute to their joint efforts at the boundary of systems and ML. With them, I hope to explore how techniques from network verification, programmable dataplanes, and disaggregated memory, combined with online telemetry, can provide end-to-end reliability guarantees for modern distributed training and serving platforms.

\textbf{Future Directions and Career Goals.}
Looking ahead, I want to turn these experiences into a coherent research agenda on systems for efficient and reliable large-scale ML. In the near term, I hope to work on runtimes and control planes that treat telemetry and ML signals as first-class inputs to scheduling, communication, and storage decisions. Longer term, I see myself in a research role-ideally as a faculty member-designing and evaluating platforms that make large-model training and serving more predictable and robust for practitioners. Training at \schoolShort{} will help me learn to pose the right questions, build principled abstractions, and evaluate them on realistic workloads, so that future ML infrastructure can offer stronger performance and reliability guarantees.


\bibliographystyle{abbrv}
\bibliography{paper}

\end{document}